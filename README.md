## üìñ Project Overview

This repository implements a complete social media sentiment analysis pipeline, from data exploration and model training to deployment as a REST API and containerization. You‚Äôll find:

* **Exploratory Notebook** (`social_media.ipynb`) demonstrating data collection, cleaning, feature extraction, model training, evaluation, and saving the artifacts. ([GitHub][1])
* **Pre-trained Artifacts**: a serialized machine learning model (`model.pkl`) and a text vectorizer (`vectorizer.pkl`) for inference. ([GitHub][1])
* **Flask Application** (`app.py`) providing a `/predict` endpoint to obtain sentiment predictions in real time. ([GitHub][1])
* **Containerization** via `Dockerfile` for easy deployment in any environment. ([GitHub][1])
* **Demonstration**: a short video (`video_demonstration.mp4`) walking through the user experience. ([GitHub][1])

The core sentiment analysis leverages **VADER** (Valence Aware Dictionary and sEntiment Reasoner), a rule-based model tailored for social media text, offering robust positive/negative/neutral scoring out of the box ([NLTK][2], [NLTK][3]).

---

## üìÇ Repository Structure

```
‚îú‚îÄ‚îÄ Dockerfile                 # Defines the cloud/container build environment  
‚îú‚îÄ‚îÄ README.md                  # (You are here)  
‚îú‚îÄ‚îÄ app.py                     # Flask API serving sentiment predictions  
‚îú‚îÄ‚îÄ model.pkl                  # Serialized trained sentiment classifier  
‚îú‚îÄ‚îÄ vectorizer.pkl             # Serialized text vectorizer (e.g., TF-IDF)  
‚îú‚îÄ‚îÄ public API endpoint        # Text file with deployed endpoint URL  
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies  
‚îú‚îÄ‚îÄ social_media.ipynb         # Jupyter notebook: EDA ‚Üí training ‚Üí evaluation  
‚îî‚îÄ‚îÄ video_demonstration.mp4    # Walkthrough of installation & usage  
```

All files and folders are listed in the repository‚Äôs root directory ([GitHub][1]).

---

## ‚öôÔ∏è Installation

1. **Clone the repository**

   ```bash
   git clone https://github.com/HoneyBohra26/Social_Media_Sentiment_Analysis.git
   cd Social_Media_Sentiment_Analysis
   ```
2. **Set up a virtual environment**

   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```
3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

   This installs core libraries such as Flask for the API, pandas and scikit-learn for data processing and modeling, and NLTK (including VADER) for sentiment scoring ([GitHub][1], [NLTK][3]).

---

## üöÄ Usage

### 1. Exploratory Notebook

Open `social_media.ipynb` in Jupyter to walk through:

* Data acquisition from social media APIs
* Text preprocessing (tokenization, stop-word removal, normalization)
* Feature extraction via `CountVectorizer`/`TfidfVectorizer`
* Model training (e.g., Logistic Regression or Naive Bayes)
* Evaluation (accuracy, confusion matrix, classification report)
* Exporting `model.pkl` and `vectorizer.pkl` for deployment. ([GitHub][1])

### 2. Running the API

```bash
python app.py
```

* **Endpoint**: `POST /predict`
* **Payload**: JSON with a `"text"` field
* **Response**: JSON `{ "sentiment": "POSITIVE"/"NEGATIVE"/"NEUTRAL", "score": float }`
  This pattern follows best practices for Flask-based model serving ([Medium][4]).

### 3. Containerization with Docker

Build and run the Docker image to deploy in any environment without local setup:

```bash
docker build -t sentiment-api .
docker run -p 5000:5000 sentiment-api
```

The container exposes the same `/predict` endpoint. ([GitHub][1])

---

## üóÇ File Descriptions

* **Dockerfile**
  Defines a lightweight Python environment, installs requirements, copies code and artifacts, and sets the Flask app as the container‚Äôs entrypoint. ([GitHub][1])

* **app.py**

  * Loads `model.pkl` and `vectorizer.pkl` at startup
  * Defines a Flask application with a prediction route
  * Handles JSON requests, applies text preprocessing, vectorizes input, and returns sentiment predictions. ([GitHub][1])

* **model.pkl**, **vectorizer.pkl**
  Serialized artifacts from the notebook, enabling the API to run inference without retraining. ([GitHub][1])

* **public API endpoint**
  A simple text file containing the live URL of the deployed Flask service for quick reference. ([GitHub][1])

* **requirements.txt**
  Pinpoints exact library versions to ensure reproducibility (Flask, scikit-learn, pandas, numpy, nltk, etc.). ([GitHub][1])

* **social\_media.ipynb**
  The core Jupyter notebook that documents end-to-end model development, from loading raw social media posts to saving final model artifacts. ([GitHub][1])

* **video\_demonstration.mp4**
  A short screencast demonstrating repository setup, running the notebook, calling the API, and interpreting results. ([GitHub][1])

---

## üîç Implementation Details

1. **Data Collection & Preprocessing**

   * Retrieve social media posts (e.g., via Twitter API).
   * Clean text: remove URLs, mentions, punctuation, lowercasing.
   * Tokenize and remove stop-words.

2. **Feature Extraction**

   * Use scikit-learn‚Äôs `CountVectorizer` or `TfidfVectorizer` to convert text to numeric feature vectors.
   * Save the fitted vectorizer to `vectorizer.pkl` for consistent inference. ([NLTK][3])

3. **Model Training**

   * Train a classifier (e.g., Logistic Regression) on labeled sentiment data.
   * Evaluate via cross-validation and metrics (accuracy, precision, recall).
   * Export the best model to `model.pkl`.

4. **API Development**

   * Build a Flask app (`app.py`) that loads both artifacts at startup.
   * Define a `/predict` POST route accepting raw text ‚Üí preprocess ‚Üí vectorize ‚Üí model.predict. ([GeeksforGeeks][5])

5. **Containerization**

   * Encapsulate the environment with a `Dockerfile` to guarantee ‚Äúit works on my machine‚Äù reliability.
